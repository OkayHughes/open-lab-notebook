<!DOCTYPE html>
<html lang="en">
  <head>
    
    
    
    <meta charset="utf-8" />
    <link rel="icon" href="https://cdn.glitch.com/8af1b7af-efae-4a35-b1d7-e47b835582bc%2Fall_TMQ_moist_day5-1.ico?v=1630525741731" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Your new 11ty blog" />
    <link rel="stylesheet" type="text/css" href="/open-lab-notebook/public/style.css"/>
    <link rel="stylesheet" type="text/css" href="/open-lab-notebook/public/fonts/cmu.css"/>
 
    <link rel="stylesheet" href="https://unpkg.com/katex@latest/dist/katex.min.css" />
    
    <title>Open Lab Notebook</title>

<meta name="description" content="A blog which serves as my lab notebook, but which is visible to the world because I&apos;m a masochist.">
<meta name="robots" content="index,follow">
<meta name="author" content="undefined">

<link rel="canonical" href="https://undefined.glitch.me/posts/ai_bullshit/ai_attn/">

<meta property="og:title" content="Open Lab Notebook">
<meta property="og:type" content="article">

<meta property="og:url" content="https://undefined.glitch.me/posts/ai_bullshit/ai_attn/">
<meta property="og:description" content="A blog which serves as my lab notebook, but which is visible to the world because I&apos;m a masochist.">
<meta property="og:image" content="https://cdn.glitch.com/8af1b7af-efae-4a35-b1d7-e47b835582bc%2Fall_TMQ_moist_day5.png?v=1630525127233">

<meta name="twitter:card" content="summary">

<meta name="twitter:url" content="https://undefined.glitch.me/posts/ai_bullshit/ai_attn/">
<meta name="twitter:title" content="Open Lab Notebook">
<meta name="twitter:description" content="A blog which serves as my lab notebook, but which is visible to the world because I&apos;m a masochist.">
<meta name="twitter:image" content="https://cdn.glitch.com/8af1b7af-efae-4a35-b1d7-e47b835582bc%2Fall_TMQ_moist_day5.png?v=1630525127233">


  </head>
  <body>
  <main>
    
    
  
    


<div class="wrapper post">
  <div>
    
    <h1 class='postTitle'>super-resolution-attention</h1>

    
    
    
    <h1>Note: This was generated by deepseek r1. I'm tryibng to see if it's useful enough to teach me things.</h1>
<h1>Next Step Tutorial: Adding Attention Mechanisms to Diffusion-Based Super-Resolution</h1>
<p>This tutorial extends our previous diffusion-based super-resolution model by incorporating attention mechanisms. We'll explain how attention works, why it's beneficial for diffusion models, and how to implement it in our existing framework.</p>
<h2>Enhanced Implementation with Attention Mechanisms</h2>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

# Set device
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Define attention mechanism
class SelfAttention(nn.Module):
    &quot;&quot;&quot;
    Self-attention mechanism that allows each position to attend to all positions
    in the feature map. This helps capture long-range dependencies.
    
    REF: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
    &quot;&quot;&quot;
    def __init__(self, channels, size):
        super(SelfAttention, self).__init__()
        self.channels = channels
        self.size = size
        
        # Query, Key, Value projections
        self.query = nn.Conv2d(channels, channels // 8, kernel_size=1)
        self.key = nn.Conv2d(channels, channels // 8, kernel_size=1)
        self.value = nn.Conv2d(channels, channels, kernel_size=1)
        
        # Gamma parameter (learnable weight for residual connection)
        self.gamma = nn.Parameter(torch.zeros(1))
        
        # Softmax for attention weights
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, x):
        &quot;&quot;&quot;
        Forward pass of self-attention.
        
        Args:
            x: Input tensor of shape (batch, channels, height, width)
            
        Returns:
            Output tensor with attention applied
        &quot;&quot;&quot;
        batch_size, C, H, W = x.shape
        
        # Generate queries, keys, values
        # REF: Q = W_Q * x, K = W_K * x, V = W_V * x
        Q = self.query(x).view(batch_size, -1, H * W).permute(0, 2, 1)  # (B, N, C')
        K = self.key(x).view(batch_size, -1, H * W)  # (B, C', N)
        V = self.value(x).view(batch_size, -1, H * W)  # (B, C, N)
        
        # Calculate attention scores
        # REF: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
        attention = torch.bmm(Q, K)  # (B, N, N)
        attention = attention / math.sqrt(self.channels // 8)  # Scale by sqrt(d_k)
        attention = self.softmax(attention)  # (B, N, N)
        
        # Apply attention to values
        out = torch.bmm(V, attention.permute(0, 2, 1))  # (B, C, N)
        out = out.view(batch_size, C, H, W)  # (B, C, H, W)
        
        # Residual connection with learnable weight
        # REF: Output = Î³ * Attention(Q, K, V) + Input
        out = self.gamma * out + x
        
        return out

# Define the enhanced U-Net with attention
class UNetWithAttention(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, base_channels=32):
        super(UNetWithAttention, self).__init__()
        
        # Encoder (downsampling)
        self.enc1 = self._block(in_channels, base_channels)
        self.enc2 = self._block(base_channels, base_channels*2)
        self.pool = nn.MaxPool2d(2)
        
        # Bottleneck with attention
        self.bottleneck = self._block(base_channels*2, base_channels*4)
        
        # Add attention mechanism at bottleneck
        # REF: Attention is most beneficial at lower resolutions where global context matters
        self.attention = SelfAttention(base_channels*4, size=(8, 8))
        
        # Decoder (upsampling)
        self.upconv2 = nn.ConvTranspose2d(base_channels*4, base_channels*2, kernel_size=2, stride=2)
        self.dec2 = self._block(base_channels*4, base_channels*2)  # Skip connection doubles channels
        self.upconv1 = nn.ConvTranspose2d(base_channels*2, base_channels, kernel_size=2, stride=2)
        self.dec1 = self._block(base_channels*2, base_channels)  # Skip connection doubles channels
        
        # Final output layer
        self.out = nn.Conv2d(base_channels, out_channels, kernel_size=1)
        
        # Time embedding layers
        self.time_embed = nn.Sequential(
            nn.Linear(1, base_channels),
            nn.SiLU(),
            nn.Linear(base_channels, base_channels)
        )
        
    def _block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.GroupNorm(8, out_channels),  # GroupNorm works better than BatchNorm for small batches
            nn.SiLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.GroupNorm(8, out_channels),
            nn.SiLU(inplace=True)
        )
    
    def forward(self, x, t):
        # Time embedding (improved)
        # REF: Project time to higher dimension and add to feature maps
        t_embed = self.time_embed(t.view(-1, 1)).unsqueeze(-1).unsqueeze(-1)
        
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        
        # Bottleneck with attention
        bottleneck = self.bottleneck(self.pool(enc2))
        
        # Add time embedding to bottleneck features
        bottleneck = bottleneck + t_embed.expand_as(bottleneck)
        
        # Apply attention
        bottleneck = self.attention(bottleneck)
        
        # Decoder with skip connections
        dec2 = self.upconv2(bottleneck)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)
        
        return self.out(dec1)

# Define the diffusion process (same as before)
class DiffusionProcess:
    def __init__(self, T=1000, beta_start=1e-4, beta_end=0.02):
        self.T = T
        
        # Linear noise schedule
        self.betas = torch.linspace(beta_start, beta_end, T)
        self.alphas = 1 - self.betas
        self.alpha_bars = torch.cumprod(self.alphas, dim=0)
        
    def forward_process(self, x0, t):
        &quot;&quot;&quot;Add noise to image at timestep t&quot;&quot;&quot;
        sqrt_alpha_bar = torch.sqrt(self.alpha_bars[t]).view(-1, 1, 1, 1).to(x0.device)
        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bars[t]).view(-1, 1, 1, 1).to(x0.device)
        epsilon = torch.randn_like(x0)
        
        xt = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * epsilon
        return xt, epsilon
    
    def reverse_process(self, model, x, t, condition=None):
        &quot;&quot;&quot;Remove noise from image at timestep t using the model&quot;&quot;&quot;
        with torch.no_grad():
            # Predict the noise
            epsilon_pred = model(x, t)
            
            # Calculate coefficients for reverse process
            alpha_t = self.alphas[t].view(-1, 1, 1, 1).to(x.device)
            alpha_bar_t = self.alpha_bars[t].view(-1, 1, 1, 1).to(x.device)
            beta_t = self.betas[t].view(-1, 1, 1, 1).to(x.device)
            
            if t &gt; 0:
                z = torch.randn_like(x)
            else:
                z = 0
                
            sigma_t = torch.sqrt(beta_t)
            x_prev = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_bar_t)) * epsilon_pred) + sigma_t * z
            
            return x_prev

# Training function (modified for attention model)
def train_diffusion_with_attention(model, diffusion, dataloader, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for i, (hr_imgs, _) in enumerate(dataloader):
            hr_imgs = hr_imgs.to(device)
            
            # Create low-resolution version (condition)
            lr_imgs = F.interpolate(hr_imgs, scale_factor=0.25, mode='bilinear')
            lr_imgs = F.interpolate(lr_imgs, size=hr_imgs.shape[2:], mode='bilinear')
            
            # Random timestep
            t = torch.randint(0, diffusion.T, (hr_imgs.size(0),), device=device).float()
            
            # Add noise to high-resolution images
            noisy_imgs, true_noise = diffusion.forward_process(hr_imgs, t)
            
            # Concatenate low-resolution condition with noisy image
            model_input = torch.cat([noisy_imgs, lr_imgs], dim=1)
            
            # Predict noise
            pred_noise = model(model_input, t)
            
            # Calculate loss
            loss = F.mse_loss(pred_noise, true_noise)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if i % 100 == 0:
                print(f&quot;Epoch {epoch+1}/{epochs}, Batch {i}, Loss: {loss.item():.4f}&quot;)
        
        print(f&quot;Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/len(dataloader):.4f}&quot;)

# Sampling function (same as before)
def sample_diffusion(model, diffusion, lr_img, img_size=(32, 32)):
    model.eval()
    
    # Start from random noise
    x = torch.randn(1, 3, *img_size).to(device)
    
    # Upsample low-resolution image to target size
    lr_img = F.interpolate(lr_img, size=img_size, mode='bilinear')
    
    # Reverse process
    for t in range(diffusion.T-1, -1, -1):
        # Create tensor of current timestep
        t_tensor = torch.tensor([t], device=device).float()
        
        # Concatenate with condition
        model_input = torch.cat([x, lr_img], dim=1)
        
        # Reverse process step
        x = diffusion.reverse_process(model, model_input, t_tensor)
        
        # Clamp values to valid range
        x = torch.clamp(x, -1.0, 1.0)
        
    return x

# Compare models function
def compare_models(original_model, attention_model, diffusion, test_img):
    &quot;&quot;&quot;Compare the performance of original and attention models&quot;&quot;&quot;
    # Create low-resolution version
    lr_img = F.interpolate(test_img, scale_factor=0.25, mode='bilinear')
    lr_img = F.interpolate(lr_img, size=test_img.shape[2:], mode='bilinear')
    
    # Generate super-resolution images
    print(&quot;Generating with original model...&quot;)
    sr_original = sample_diffusion(original_model, diffusion, lr_img, img_size=test_img.shape[2:])
    
    print(&quot;Generating with attention model...&quot;)
    sr_attention = sample_diffusion(attention_model, diffusion, lr_img, img_size=test_img.shape[2:])
    
    # Visualize results
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))
    
    axes[0].imshow(test_img[0].cpu().permute(1, 2, 0) * 0.5 + 0.5)
    axes[0].set_title(&quot;Original HR Image&quot;)
    axes[0].axis('off')
    
    axes[1].imshow(lr_img[0].cpu().permute(1, 2, 0) * 0.5 + 0.5)
    axes[1].set_title(&quot;LR Input&quot;)
    axes[1].axis('off')
    
    axes[2].imshow(sr_original[0].cpu().permute(1, 2, 0) * 0.5 + 0.5)
    axes[2].set_title(&quot;Original Model&quot;)
    axes[2].axis('off')
    
    axes[3].imshow(sr_attention[0].cpu().permute(1, 2, 0) * 0.5 + 0.5)
    axes[3].set_title(&quot;Attention Model&quot;)
    axes[3].axis('off')
    
    plt.tight_layout()
    plt.savefig('comparison_result.png')
    plt.show()

# Main execution
if __name__ == &quot;__main__&quot;:
    # Hyperparameters
    batch_size = 32
    learning_rate = 1e-4
    epochs = 5
    T = 1000  # Number of diffusion steps
    
    # Load and preprocess data
    transform = transforms.Compose([
        transforms.Resize((32, 32)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # Initialize models and diffusion process
    original_model = UNet(in_channels=6, out_channels=3).to(device)
    attention_model = UNetWithAttention(in_channels=6, out_channels=3).to(device)
    diffusion = DiffusionProcess(T=T)
    
    # Train both models
    print(&quot;Training original model...&quot;)
    optimizer_original = torch.optim.Adam(original_model.parameters(), lr=learning_rate)
    train_diffusion(original_model, diffusion, train_loader, optimizer_original, epochs=epochs)
    
    print(&quot;Training attention model...&quot;)
    optimizer_attention = torch.optim.Adam(attention_model.parameters(), lr=learning_rate)
    train_diffusion_with_attention(attention_model, diffusion, train_loader, optimizer_attention, epochs=epochs)
    
    # Test with a sample image
    test_img, _ = next(iter(train_loader))
    test_img = test_img[0:1].to(device)  # Take one image
    
    # Compare models
    compare_models(original_model, attention_model, diffusion, test_img)
</code></pre>
<h2>Tutorial: Understanding Attention Mechanisms in Diffusion Models</h2>
<h3>1. Introduction to Attention Mechanisms</h3>
<p>Attention mechanisms allow models to dynamically focus on different parts of their input when making predictions. In the context of diffusion models for super-resolution, attention helps the model:</p>
<ol>
<li><strong>Focus on relevant regions</strong>: Identify which parts of the low-resolution image need more refinement</li>
<li><strong>Capture long-range dependencies</strong>: Understand relationships between distant pixels</li>
<li><strong>Handle varying levels of detail</strong>: Adaptively process different image regions based on their complexity</li>
</ol>
<h3>2. Mathematical Foundation of Self-Attention</h3>
<p>The self-attention mechanism is based on three learned projections:</p>
<h4>Query, Key, and Value</h4>
<p>For an input feature map (X \in \mathbb{R}^{C \times H \times W}):</p>
<p>[
Q = W_Q \cdot X, \quad K = W_K \cdot X, \quad V = W_V \cdot X
]</p>
<p>Where:</p>
<ul>
<li>(W_Q, W_K, W_V) are learned weight matrices</li>
<li>(Q, K, V) are the query, key, and value representations</li>
</ul>
<h4>Attention Calculation</h4>
<p>The attention weights are computed as:</p>
<p>[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
]</p>
<p>Where:</p>
<ul>
<li>(d_k) is the dimension of the key vectors (used for scaling)</li>
<li>The softmax ensures the attention weights sum to 1</li>
</ul>
<p><strong>Code Reference</strong>: In the <code>SelfAttention.forward</code> method:</p>
<pre><code class="language-python"># Calculate attention scores
attention = torch.bmm(Q, K)  # (B, N, N)
attention = attention / math.sqrt(self.channels // 8)  # Scale by sqrt(d_k)
attention = self.softmax(attention)  # (B, N, N)

# Apply attention to values
out = torch.bmm(V, attention.permute(0, 2, 1))  # (B, C, N)
</code></pre>
<h4>Residual Connection</h4>
<p>To ensure stable training, we use a residual connection:</p>
<p>[
\text{Output} = \gamma \cdot \text{Attention}(Q, K, V) + X
]</p>
<p>Where (\gamma) is a learnable parameter that starts at 0 and is gradually learned during training.</p>
<p><strong>Code Reference</strong>: In the <code>SelfAttention.forward</code> method:</p>
<pre><code class="language-python"># Residual connection with learnable weight
out = self.gamma * out + x
</code></pre>
<h3>3. Integrating Attention into the Diffusion Model</h3>
<h4>Placement of Attention Blocks</h4>
<p>Attention is most effective when placed in the bottleneck of the U-Net, where:</p>
<ol>
<li>The feature maps are at their lowest spatial resolution</li>
<li>The receptive field is largest</li>
<li>Global context is most important</li>
</ol>
<p><strong>Code Reference</strong>: In the <code>UNetWithAttention</code> class:</p>
<pre><code class="language-python"># Bottleneck with attention
self.bottleneck = self._block(base_channels*2, base_channels*4)

# Add attention mechanism at bottleneck
self.attention = SelfAttention(base_channels*4, size=(8, 8))
</code></pre>
<h4>Enhanced Time Embedding</h4>
<p>We've also improved the time embedding by:</p>
<ol>
<li>Projecting the time step to a higher dimension</li>
<li>Using SiLU activation functions for smoother gradients</li>
<li>Adding the time embedding to the feature maps</li>
</ol>
<p><strong>Code Reference</strong>: In the <code>UNetWithAttention</code> class:</p>
<pre><code class="language-python"># Time embedding layers
self.time_embed = nn.Sequential(
    nn.Linear(1, base_channels),
    nn.SiLU(),
    nn.Linear(base_channels, base_channels)
)

# Add time embedding to bottleneck features
bottleneck = bottleneck + t_embed.expand_as(bottleneck)
</code></pre>
<h3>4. Benefits of Attention in Diffusion Models</h3>
<h4>Improved Coherence</h4>
<p>Attention helps maintain coherence across the generated image by allowing different regions to &quot;communicate&quot; with each other. This is particularly important for super-resolution, where we need to ensure that the enhanced details are consistent across the entire image.</p>
<h4>Better Handling of Complex Patterns</h4>
<p>Attention mechanisms excel at capturing complex, long-range dependencies that are difficult for convolutional networks to learn. This makes them particularly well-suited for:</p>
<ol>
<li><strong>Texture synthesis</strong>: Generating realistic textures that match the input</li>
<li><strong>Edge preservation</strong>: Maintaining sharp edges while reducing artifacts</li>
<li><strong>Detail enhancement</strong>: Adding plausible high-frequency details</li>
</ol>
<h4>Adaptive Processing</h4>
<p>Different image regions require different levels of processing. Attention allows the model to:</p>
<ol>
<li><strong>Focus on difficult areas</strong>: Spend more computational resources on complex regions</li>
<li><strong>Ignore simple areas</strong>: Process uniform regions more efficiently</li>
<li><strong>Balance global and local information</strong>: Combine both types of information effectively</li>
</ol>
<h3>5. Practical Implementation Considerations</h3>
<h4>Computational Cost</h4>
<p>Attention mechanisms increase computational requirements:</p>
<ul>
<li>Memory: (O(N^2)) where N is the number of pixels</li>
<li>Computation: (O(N^2)) for the attention matrix multiplication</li>
</ul>
<p><strong>Mitigation strategies</strong>:</p>
<ol>
<li>Use attention only at lower resolutions</li>
<li>Implement efficient attention variants (e.g., linear attention)</li>
<li>Use attention in the bottleneck only</li>
</ol>
<h4>Training Stability</h4>
<p>Attention can sometimes make training less stable. We address this by:</p>
<ol>
<li>Using residual connections with learnable weights</li>
<li>Initializing the gamma parameter to 0</li>
<li>Using GroupNorm instead of BatchNorm</li>
</ol>
<p><strong>Code Reference</strong>: In the <code>UNetWithAttention._block</code> method:</p>
<pre><code class="language-python">nn.GroupNorm(8, out_channels),  # GroupNorm works better than BatchNorm for small batches
</code></pre>
<h3>6. Expected Improvements</h3>
<p>With the addition of attention mechanisms, we expect to see:</p>
<ol>
<li><strong>Higher quality results</strong>: More realistic and detailed super-resolved images</li>
<li><strong>Better coherence</strong>: Improved consistency across the generated image</li>
<li><strong>Reduced artifacts</strong>: Fewer unnatural patterns or distortions</li>
<li><strong>Faster convergence</strong>: More efficient learning of complex patterns</li>
</ol>
<h3>7. Further Enhancements</h3>
<p>Beyond the basic attention mechanism implemented here, you could explore:</p>
<ol>
<li><strong>Multi-head attention</strong>: Allow the model to focus on different aspects simultaneously</li>
<li><strong>Cross-attention</strong>: Specifically attend to the low-resolution conditioning image</li>
<li><strong>Efficient attention variants</strong>: Reduce computational requirements</li>
<li><strong>Attention at multiple scales</strong>: Apply attention at different levels of the U-Net</li>
</ol>
<p>This enhanced implementation demonstrates how attention mechanisms can be integrated into diffusion models for super-resolution, providing both theoretical understanding and practical implementation details.</p>

    <div class="controls">

    
      
          <h3>
            Parent post:
      </h3>
          <ul><li><a href="/open-lab-notebook/posts/ai_bullshit/ai/">ai bullshit</a></li></ul>
      
      
      
      
      
    </div>
  </div>
</div>


      <footer class="footer">
        <div class="links">
          <a href="/">Home</a>
          <span class="divider">|</span>
          <a href="/posts">Posts</a>
          <span class="divider">|</span>
          <a href="/about">About</a>
        </div>
        <a class="btn--remix" target="_top" href="https://glitch.com/edit/#!/remix/glitch-hello-eleventy">
        <img src="https://cdn.glitch.com/605e2a51-d45f-4d87-a285-9410ad350515%2FLogo_Color.svg?v=1618199565140" alt="" /> 
        Remix on Glitch
        </a>
        <a class="btn--remix" target="_top" href="https://glitch.com/edit/#!/open-lab-notebook">
        Edit
        </a>
      </footer>
    
  </main>
   
  </body>
</html>
