<!DOCTYPE html>
<html lang="en">
  <head>
    
    
    
    <meta charset="utf-8" />
    <link rel="icon" href="https://cdn.glitch.com/8af1b7af-efae-4a35-b1d7-e47b835582bc%2Fall_TMQ_moist_day5-1.ico?v=1630525741731" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Your new 11ty blog" />
    <link rel="stylesheet" type="text/css" href="/open-lab-notebook/public/style.css"/>
    <link rel="stylesheet" type="text/css" href="/open-lab-notebook/public/fonts/cmu.css"/>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/9000.0.1/themes/prism-twilight.min.css" rel="stylesheet" />
 
    <link rel="stylesheet" href="https://unpkg.com/katex@latest/dist/katex.min.css" />
    
    <title>Open Lab Notebook</title>

<meta name="description" content="A blog which serves as my lab notebook, but which is visible to the world because I&apos;m a masochist.">
<meta name="robots" content="index,follow">
<meta name="author" content="undefined">

<link rel="canonical" href="https://undefined.glitch.me/posts/ai_bullshit/ai_attn/">

<meta property="og:title" content="Open Lab Notebook">
<meta property="og:type" content="article">

<meta property="og:url" content="https://undefined.glitch.me/posts/ai_bullshit/ai_attn/">
<meta property="og:description" content="A blog which serves as my lab notebook, but which is visible to the world because I&apos;m a masochist.">
<meta property="og:image" content="https://cdn.glitch.com/8af1b7af-efae-4a35-b1d7-e47b835582bc%2Fall_TMQ_moist_day5.png?v=1630525127233">

<meta name="twitter:card" content="summary">

<meta name="twitter:url" content="https://undefined.glitch.me/posts/ai_bullshit/ai_attn/">
<meta name="twitter:title" content="Open Lab Notebook">
<meta name="twitter:description" content="A blog which serves as my lab notebook, but which is visible to the world because I&apos;m a masochist.">
<meta name="twitter:image" content="https://cdn.glitch.com/8af1b7af-efae-4a35-b1d7-e47b835582bc%2Fall_TMQ_moist_day5.png?v=1630525127233">


  </head>
  <body>
  <main>
    
    
  
    


<div class="wrapper post">
  <div>
    
    <h1 class='postTitle'>super-resolution-attention</h1>

    
    
    
    <h1>Note: This was generated by deepseek r1. I'm tryibng to see if it's useful enough to teach me things.</h1>
<h1>Next Step Tutorial: Adding Attention Mechanisms to Diffusion-Based Super-Resolution</h1>
<p>This tutorial extends our previous diffusion-based super-resolution model by incorporating attention mechanisms. We'll explain how attention works, why it's beneficial for diffusion models, and how to implement it in our existing framework.</p>
<h2>Enhanced Implementation with Attention Mechanisms</h2>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> math
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># Set device</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>

<span class="token comment"># Define attention mechanism</span>
<span class="token keyword">class</span> <span class="token class-name">SelfAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Self-attention mechanism that allows each position to attend to all positions
    in the feature map. This helps capture long-range dependencies.
    
    REF: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SelfAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>channels <span class="token operator">=</span> channels
        self<span class="token punctuation">.</span>size <span class="token operator">=</span> size
        
        <span class="token comment"># Query, Key, Value projections</span>
        self<span class="token punctuation">.</span>query <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>key <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>value <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Gamma parameter (learnable weight for residual connection)</span>
        self<span class="token punctuation">.</span>gamma <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Softmax for attention weights</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Forward pass of self-attention.
        
        Args:
            x: Input tensor of shape (batch, channels, height, width)
            
        Returns:
            Output tensor with attention applied
        """</span>
        batch_size<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        
        <span class="token comment"># Generate queries, keys, values</span>
        <span class="token comment"># REF: Q = W_Q * x, K = W_K * x, V = W_V * x</span>
        Q <span class="token operator">=</span> self<span class="token punctuation">.</span>query<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> H <span class="token operator">*</span> W<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (B, N, C')</span>
        K <span class="token operator">=</span> self<span class="token punctuation">.</span>key<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> H <span class="token operator">*</span> W<span class="token punctuation">)</span>  <span class="token comment"># (B, C', N)</span>
        V <span class="token operator">=</span> self<span class="token punctuation">.</span>value<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> H <span class="token operator">*</span> W<span class="token punctuation">)</span>  <span class="token comment"># (B, C, N)</span>
        
        <span class="token comment"># Calculate attention scores</span>
        <span class="token comment"># REF: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V</span>
        attention <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">)</span>  <span class="token comment"># (B, N, N)</span>
        attention <span class="token operator">=</span> attention <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>channels <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">)</span>  <span class="token comment"># Scale by sqrt(d_k)</span>
        attention <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention<span class="token punctuation">)</span>  <span class="token comment"># (B, N, N)</span>
        
        <span class="token comment"># Apply attention to values</span>
        out <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>V<span class="token punctuation">,</span> attention<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># (B, C, N)</span>
        out <span class="token operator">=</span> out<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>  <span class="token comment"># (B, C, H, W)</span>
        
        <span class="token comment"># Residual connection with learnable weight</span>
        <span class="token comment"># REF: Output = γ * Attention(Q, K, V) + Input</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> out <span class="token operator">+</span> x
        
        <span class="token keyword">return</span> out

<span class="token comment"># Define the enhanced U-Net with attention</span>
<span class="token keyword">class</span> <span class="token class-name">UNetWithAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> base_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>UNetWithAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Encoder (downsampling)</span>
        self<span class="token punctuation">.</span>enc1 <span class="token operator">=</span> self<span class="token punctuation">.</span>_block<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> base_channels<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>enc2 <span class="token operator">=</span> self<span class="token punctuation">.</span>_block<span class="token punctuation">(</span>base_channels<span class="token punctuation">,</span> base_channels<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Bottleneck with attention</span>
        self<span class="token punctuation">.</span>bottleneck <span class="token operator">=</span> self<span class="token punctuation">.</span>_block<span class="token punctuation">(</span>base_channels<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> base_channels<span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Add attention mechanism at bottleneck</span>
        <span class="token comment"># REF: Attention is most beneficial at lower resolutions where global context matters</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> SelfAttention<span class="token punctuation">(</span>base_channels<span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Decoder (upsampling)</span>
        self<span class="token punctuation">.</span>upconv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>base_channels<span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> base_channels<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dec2 <span class="token operator">=</span> self<span class="token punctuation">.</span>_block<span class="token punctuation">(</span>base_channels<span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> base_channels<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># Skip connection doubles channels</span>
        self<span class="token punctuation">.</span>upconv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>base_channels<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> base_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dec1 <span class="token operator">=</span> self<span class="token punctuation">.</span>_block<span class="token punctuation">(</span>base_channels<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> base_channels<span class="token punctuation">)</span>  <span class="token comment"># Skip connection doubles channels</span>
        
        <span class="token comment"># Final output layer</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>base_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Time embedding layers</span>
        self<span class="token punctuation">.</span>time_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> base_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>base_channels<span class="token punctuation">,</span> base_channels<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">_block</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># GroupNorm works better than BatchNorm for small batches</span>
            nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Time embedding (improved)</span>
        <span class="token comment"># REF: Project time to higher dimension and add to feature maps</span>
        t_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>time_embed<span class="token punctuation">(</span>t<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Encoder</span>
        enc1 <span class="token operator">=</span> self<span class="token punctuation">.</span>enc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        enc2 <span class="token operator">=</span> self<span class="token punctuation">.</span>enc2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>enc1<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Bottleneck with attention</span>
        bottleneck <span class="token operator">=</span> self<span class="token punctuation">.</span>bottleneck<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>enc2<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Add time embedding to bottleneck features</span>
        bottleneck <span class="token operator">=</span> bottleneck <span class="token operator">+</span> t_embed<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>bottleneck<span class="token punctuation">)</span>
        
        <span class="token comment"># Apply attention</span>
        bottleneck <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>bottleneck<span class="token punctuation">)</span>
        
        <span class="token comment"># Decoder with skip connections</span>
        dec2 <span class="token operator">=</span> self<span class="token punctuation">.</span>upconv2<span class="token punctuation">(</span>bottleneck<span class="token punctuation">)</span>
        dec2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>dec2<span class="token punctuation">,</span> enc2<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        dec2 <span class="token operator">=</span> self<span class="token punctuation">.</span>dec2<span class="token punctuation">(</span>dec2<span class="token punctuation">)</span>
        
        dec1 <span class="token operator">=</span> self<span class="token punctuation">.</span>upconv1<span class="token punctuation">(</span>dec2<span class="token punctuation">)</span>
        dec1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>dec1<span class="token punctuation">,</span> enc1<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        dec1 <span class="token operator">=</span> self<span class="token punctuation">.</span>dec1<span class="token punctuation">(</span>dec1<span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>dec1<span class="token punctuation">)</span>

<span class="token comment"># Define the diffusion process (same as before)</span>
<span class="token keyword">class</span> <span class="token class-name">DiffusionProcess</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> T<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> beta_start<span class="token operator">=</span><span class="token number">1e-4</span><span class="token punctuation">,</span> beta_end<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>T <span class="token operator">=</span> T
        
        <span class="token comment"># Linear noise schedule</span>
        self<span class="token punctuation">.</span>betas <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>beta_start<span class="token punctuation">,</span> beta_end<span class="token punctuation">,</span> T<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>alphas <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>betas
        self<span class="token punctuation">.</span>alpha_bars <span class="token operator">=</span> torch<span class="token punctuation">.</span>cumprod<span class="token punctuation">(</span>self<span class="token punctuation">.</span>alphas<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward_process</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x0<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Add noise to image at timestep t"""</span>
        sqrt_alpha_bar <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>alpha_bars<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x0<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        sqrt_one_minus_alpha_bar <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>alpha_bars<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x0<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        epsilon <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>x0<span class="token punctuation">)</span>
        
        xt <span class="token operator">=</span> sqrt_alpha_bar <span class="token operator">*</span> x0 <span class="token operator">+</span> sqrt_one_minus_alpha_bar <span class="token operator">*</span> epsilon
        <span class="token keyword">return</span> xt<span class="token punctuation">,</span> epsilon
    
    <span class="token keyword">def</span> <span class="token function">reverse_process</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> x<span class="token punctuation">,</span> t<span class="token punctuation">,</span> condition<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Remove noise from image at timestep t using the model"""</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># Predict the noise</span>
            epsilon_pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
            
            <span class="token comment"># Calculate coefficients for reverse process</span>
            alpha_t <span class="token operator">=</span> self<span class="token punctuation">.</span>alphas<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            alpha_bar_t <span class="token operator">=</span> self<span class="token punctuation">.</span>alpha_bars<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            beta_t <span class="token operator">=</span> self<span class="token punctuation">.</span>betas<span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            
            <span class="token keyword">if</span> t <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
                z <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                z <span class="token operator">=</span> <span class="token number">0</span>
                
            sigma_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>beta_t<span class="token punctuation">)</span>
            x_prev <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>alpha_t<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> <span class="token punctuation">(</span>beta_t <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> alpha_bar_t<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> epsilon_pred<span class="token punctuation">)</span> <span class="token operator">+</span> sigma_t <span class="token operator">*</span> z
            
            <span class="token keyword">return</span> x_prev

<span class="token comment"># Training function (modified for attention model)</span>
<span class="token keyword">def</span> <span class="token function">train_diffusion_with_attention</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> diffusion<span class="token punctuation">,</span> dataloader<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        total_loss <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>hr_imgs<span class="token punctuation">,</span> _<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>
            hr_imgs <span class="token operator">=</span> hr_imgs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            
            <span class="token comment"># Create low-resolution version (condition)</span>
            lr_imgs <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>hr_imgs<span class="token punctuation">,</span> scale_factor<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>
            lr_imgs <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>lr_imgs<span class="token punctuation">,</span> size<span class="token operator">=</span>hr_imgs<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>
            
            <span class="token comment"># Random timestep</span>
            t <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> diffusion<span class="token punctuation">.</span>T<span class="token punctuation">,</span> <span class="token punctuation">(</span>hr_imgs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
            
            <span class="token comment"># Add noise to high-resolution images</span>
            noisy_imgs<span class="token punctuation">,</span> true_noise <span class="token operator">=</span> diffusion<span class="token punctuation">.</span>forward_process<span class="token punctuation">(</span>hr_imgs<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
            
            <span class="token comment"># Concatenate low-resolution condition with noisy image</span>
            model_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>noisy_imgs<span class="token punctuation">,</span> lr_imgs<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            
            <span class="token comment"># Predict noise</span>
            pred_noise <span class="token operator">=</span> model<span class="token punctuation">(</span>model_input<span class="token punctuation">,</span> t<span class="token punctuation">)</span>
            
            <span class="token comment"># Calculate loss</span>
            loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>pred_noise<span class="token punctuation">,</span> true_noise<span class="token punctuation">)</span>
            
            <span class="token comment"># Backward pass</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            
            total_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            
            <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>epochs<span class="token punctuation">}</span></span><span class="token string">, Batch </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">, Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
        
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>epochs<span class="token punctuation">}</span></span><span class="token string">, Average Loss: </span><span class="token interpolation"><span class="token punctuation">{</span>total_loss<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># Sampling function (same as before)</span>
<span class="token keyword">def</span> <span class="token function">sample_diffusion</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> diffusion<span class="token punctuation">,</span> lr_img<span class="token punctuation">,</span> img_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># Start from random noise</span>
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">*</span>img_size<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    
    <span class="token comment"># Upsample low-resolution image to target size</span>
    lr_img <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>lr_img<span class="token punctuation">,</span> size<span class="token operator">=</span>img_size<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>
    
    <span class="token comment"># Reverse process</span>
    <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>diffusion<span class="token punctuation">.</span>T<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Create tensor of current timestep</span>
        t_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>t<span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Concatenate with condition</span>
        model_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span> lr_img<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        
        <span class="token comment"># Reverse process step</span>
        x <span class="token operator">=</span> diffusion<span class="token punctuation">.</span>reverse_process<span class="token punctuation">(</span>model<span class="token punctuation">,</span> model_input<span class="token punctuation">,</span> t_tensor<span class="token punctuation">)</span>
        
        <span class="token comment"># Clamp values to valid range</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span>
        
    <span class="token keyword">return</span> x

<span class="token comment"># Compare models function</span>
<span class="token keyword">def</span> <span class="token function">compare_models</span><span class="token punctuation">(</span>original_model<span class="token punctuation">,</span> attention_model<span class="token punctuation">,</span> diffusion<span class="token punctuation">,</span> test_img<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Compare the performance of original and attention models"""</span>
    <span class="token comment"># Create low-resolution version</span>
    lr_img <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>test_img<span class="token punctuation">,</span> scale_factor<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>
    lr_img <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>lr_img<span class="token punctuation">,</span> size<span class="token operator">=</span>test_img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">)</span>
    
    <span class="token comment"># Generate super-resolution images</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Generating with original model..."</span><span class="token punctuation">)</span>
    sr_original <span class="token operator">=</span> sample_diffusion<span class="token punctuation">(</span>original_model<span class="token punctuation">,</span> diffusion<span class="token punctuation">,</span> lr_img<span class="token punctuation">,</span> img_size<span class="token operator">=</span>test_img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Generating with attention model..."</span><span class="token punctuation">)</span>
    sr_attention <span class="token operator">=</span> sample_diffusion<span class="token punctuation">(</span>attention_model<span class="token punctuation">,</span> diffusion<span class="token punctuation">,</span> lr_img<span class="token punctuation">,</span> img_size<span class="token operator">=</span>test_img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    
    <span class="token comment"># Visualize results</span>
    fig<span class="token punctuation">,</span> axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>test_img<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.5</span> <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span>
    axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Original HR Image"</span><span class="token punctuation">)</span>
    axes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>
    
    axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>lr_img<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.5</span> <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span>
    axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"LR Input"</span><span class="token punctuation">)</span>
    axes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>
    
    axes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>sr_original<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.5</span> <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span>
    axes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Original Model"</span><span class="token punctuation">)</span>
    axes<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>
    
    axes<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>sr_attention<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.5</span> <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span>
    axes<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Attention Model"</span><span class="token punctuation">)</span>
    axes<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">'off'</span><span class="token punctuation">)</span>
    
    plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'comparison_result.png'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Main execution</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token comment"># Hyperparameters</span>
    batch_size <span class="token operator">=</span> <span class="token number">32</span>
    learning_rate <span class="token operator">=</span> <span class="token number">1e-4</span>
    epochs <span class="token operator">=</span> <span class="token number">5</span>
    T <span class="token operator">=</span> <span class="token number">1000</span>  <span class="token comment"># Number of diffusion steps</span>
    
    <span class="token comment"># Load and preprocess data</span>
    transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
        transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span>
    
    train_dataset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>
    train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    <span class="token comment"># Initialize models and diffusion process</span>
    original_model <span class="token operator">=</span> UNet<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    attention_model <span class="token operator">=</span> UNetWithAttention<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    diffusion <span class="token operator">=</span> DiffusionProcess<span class="token punctuation">(</span>T<span class="token operator">=</span>T<span class="token punctuation">)</span>
    
    <span class="token comment"># Train both models</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Training original model..."</span><span class="token punctuation">)</span>
    optimizer_original <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>original_model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>
    train_diffusion<span class="token punctuation">(</span>original_model<span class="token punctuation">,</span> diffusion<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> optimizer_original<span class="token punctuation">,</span> epochs<span class="token operator">=</span>epochs<span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Training attention model..."</span><span class="token punctuation">)</span>
    optimizer_attention <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>attention_model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>
    train_diffusion_with_attention<span class="token punctuation">(</span>attention_model<span class="token punctuation">,</span> diffusion<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> optimizer_attention<span class="token punctuation">,</span> epochs<span class="token operator">=</span>epochs<span class="token punctuation">)</span>
    
    <span class="token comment"># Test with a sample image</span>
    test_img<span class="token punctuation">,</span> _ <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">)</span>
    test_img <span class="token operator">=</span> test_img<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  <span class="token comment"># Take one image</span>
    
    <span class="token comment"># Compare models</span>
    compare_models<span class="token punctuation">(</span>original_model<span class="token punctuation">,</span> attention_model<span class="token punctuation">,</span> diffusion<span class="token punctuation">,</span> test_img<span class="token punctuation">)</span></code></pre>
<h2>Tutorial: Understanding Attention Mechanisms in Diffusion Models</h2>
<h3>1. Introduction to Attention Mechanisms</h3>
<p>Attention mechanisms allow models to dynamically focus on different parts of their input when making predictions. In the context of diffusion models for super-resolution, attention helps the model:</p>
<ol>
<li><strong>Focus on relevant regions</strong>: Identify which parts of the low-resolution image need more refinement</li>
<li><strong>Capture long-range dependencies</strong>: Understand relationships between distant pixels</li>
<li><strong>Handle varying levels of detail</strong>: Adaptively process different image regions based on their complexity</li>
</ol>
<h3>2. Mathematical Foundation of Self-Attention</h3>
<p>The self-attention mechanism is based on three learned projections:</p>
<h4>Query, Key, and Value</h4>
<p>For an input feature map <code><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X \in \mathbb{R}^{C \times H \times W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span></span></span></span></span></span></span></span></code>:</p>
<p><code><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo>=</mo><msub><mi>W</mi><mi>Q</mi></msub><mo>⋅</mo><mi>X</mi><mo separator="true">,</mo><mspace width="1em"/><mi>K</mi><mo>=</mo><msub><mi>W</mi><mi>K</mi></msub><mo>⋅</mo><mi>X</mi><mo separator="true">,</mo><mspace width="1em"/><mi>V</mi><mo>=</mo><msub><mi>W</mi><mi>V</mi></msub><mo>⋅</mo><mi>X</mi></mrow><annotation encoding="application/x-tex"> Q = W_Q \cdot X, \quad K = W_K \cdot X, \quad V = W_V \cdot X </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span></code></p>
<p>Where:</p>
<ul>
<li><code><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>K</mi></msub><mo separator="true">,</mo><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_Q, W_K, W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></code> are learned weight matrices</li>
<li><code><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Q, K, V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></code> are the query, key, and value representations</li>
</ul>
<h4>Attention Calculation</h4>
<p>The attention weights are computed as:</p>
<p><code><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex"> \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4684em;vertical-align:-0.95em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></code></p>
<p>Where:</p>
<ul>
<li><code><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></code> is the dimension of the key vectors (used for scaling)</li>
<li>The softmax ensures the attention weights sum to 1</li>
</ul>
<p><strong>Code Reference</strong>: In the <code>SelfAttention.forward</code> method:</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Calculate attention scores</span>
attention <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">)</span>  <span class="token comment"># (B, N, N)</span>
attention <span class="token operator">=</span> attention <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>channels <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">)</span>  <span class="token comment"># Scale by sqrt(d_k)</span>
attention <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention<span class="token punctuation">)</span>  <span class="token comment"># (B, N, N)</span>

<span class="token comment"># Apply attention to values</span>
out <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>V<span class="token punctuation">,</span> attention<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># (B, C, N)</span></code></pre>
<h4>Residual Connection</h4>
<p>To ensure stable training, we use a residual connection:</p>
<p><code><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Output</mtext><mo>=</mo><mi>γ</mi><mo>⋅</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>+</mo><mi>X</mi></mrow><annotation encoding="application/x-tex"> \text{Output} = \gamma \cdot \text{Attention}(Q, K, V) + X </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord text"><span class="mord">Output</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span></span></code></p>
<p>Where <code><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span></code> is a learnable parameter that starts at 0 and is gradually learned during training.</p>
<p><strong>Code Reference</strong>: In the <code>SelfAttention.forward</code> method:</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Residual connection with learnable weight</span>
out <span class="token operator">=</span> self<span class="token punctuation">.</span>gamma <span class="token operator">*</span> out <span class="token operator">+</span> x</code></pre>
<h3>3. Integrating Attention into the Diffusion Model</h3>
<h4>Placement of Attention Blocks</h4>
<p>Attention is most effective when placed in the bottleneck of the U-Net, where:</p>
<ol>
<li>The feature maps are at their lowest spatial resolution</li>
<li>The receptive field is largest</li>
<li>Global context is most important</li>
</ol>
<p><strong>Code Reference</strong>: In the <code>UNetWithAttention</code> class:</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Bottleneck with attention</span>
self<span class="token punctuation">.</span>bottleneck <span class="token operator">=</span> self<span class="token punctuation">.</span>_block<span class="token punctuation">(</span>base_channels<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> base_channels<span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">)</span>

<span class="token comment"># Add attention mechanism at bottleneck</span>
self<span class="token punctuation">.</span>attention <span class="token operator">=</span> SelfAttention<span class="token punctuation">(</span>base_channels<span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<h4>Enhanced Time Embedding</h4>
<p>We've also improved the time embedding by:</p>
<ol>
<li>Projecting the time step to a higher dimension</li>
<li>Using SiLU activation functions for smoother gradients</li>
<li>Adding the time embedding to the feature maps</li>
</ol>
<p><strong>Code Reference</strong>: In the <code>UNetWithAttention</code> class:</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Time embedding layers</span>
self<span class="token punctuation">.</span>time_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> base_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>SiLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>base_channels<span class="token punctuation">,</span> base_channels<span class="token punctuation">)</span>
<span class="token punctuation">)</span>

<span class="token comment"># Add time embedding to bottleneck features</span>
bottleneck <span class="token operator">=</span> bottleneck <span class="token operator">+</span> t_embed<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>bottleneck<span class="token punctuation">)</span></code></pre>
<h3>4. Benefits of Attention in Diffusion Models</h3>
<h4>Improved Coherence</h4>
<p>Attention helps maintain coherence across the generated image by allowing different regions to &quot;communicate&quot; with each other. This is particularly important for super-resolution, where we need to ensure that the enhanced details are consistent across the entire image.</p>
<h4>Better Handling of Complex Patterns</h4>
<p>Attention mechanisms excel at capturing complex, long-range dependencies that are difficult for convolutional networks to learn. This makes them particularly well-suited for:</p>
<ol>
<li><strong>Texture synthesis</strong>: Generating realistic textures that match the input</li>
<li><strong>Edge preservation</strong>: Maintaining sharp edges while reducing artifacts</li>
<li><strong>Detail enhancement</strong>: Adding plausible high-frequency details</li>
</ol>
<h4>Adaptive Processing</h4>
<p>Different image regions require different levels of processing. Attention allows the model to:</p>
<ol>
<li><strong>Focus on difficult areas</strong>: Spend more computational resources on complex regions</li>
<li><strong>Ignore simple areas</strong>: Process uniform regions more efficiently</li>
<li><strong>Balance global and local information</strong>: Combine both types of information effectively</li>
</ol>
<h3>5. Practical Implementation Considerations</h3>
<h4>Computational Cost</h4>
<p>Attention mechanisms increase computational requirements:</p>
<ul>
<li>Memory: <code><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></code> where N is the number of pixels</li>
<li>Computation: <code><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></code> for the attention matrix multiplication</li>
</ul>
<p><strong>Mitigation strategies</strong>:</p>
<ol>
<li>Use attention only at lower resolutions</li>
<li>Implement efficient attention variants (e.g., linear attention)</li>
<li>Use attention in the bottleneck only</li>
</ol>
<h4>Training Stability</h4>
<p>Attention can sometimes make training less stable. We address this by:</p>
<ol>
<li>Using residual connections with learnable weights</li>
<li>Initializing the gamma parameter to 0</li>
<li>Using GroupNorm instead of BatchNorm</li>
</ol>
<p><strong>Code Reference</strong>: In the <code>UNetWithAttention._block</code> method:</p>
<pre class="language-python"><code class="language-python">nn<span class="token punctuation">.</span>GroupNorm<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># GroupNorm works better than BatchNorm for small batches</span></code></pre>
<h3>6. Expected Improvements</h3>
<p>With the addition of attention mechanisms, we expect to see:</p>
<ol>
<li><strong>Higher quality results</strong>: More realistic and detailed super-resolved images</li>
<li><strong>Better coherence</strong>: Improved consistency across the generated image</li>
<li><strong>Reduced artifacts</strong>: Fewer unnatural patterns or distortions</li>
<li><strong>Faster convergence</strong>: More efficient learning of complex patterns</li>
</ol>
<h3>7. Further Enhancements</h3>
<p>Beyond the basic attention mechanism implemented here, you could explore:</p>
<ol>
<li><strong>Multi-head attention</strong>: Allow the model to focus on different aspects simultaneously</li>
<li><strong>Cross-attention</strong>: Specifically attend to the low-resolution conditioning image</li>
<li><strong>Efficient attention variants</strong>: Reduce computational requirements</li>
<li><strong>Attention at multiple scales</strong>: Apply attention at different levels of the U-Net</li>
</ol>
<p>This enhanced implementation demonstrates how attention mechanisms can be integrated into diffusion models for super-resolution, providing both theoretical understanding and practical implementation details.</p>

    <div class="controls">

    
      
          <h3>
            Parent post:
      </h3>
          <ul><li><a href="/open-lab-notebook/posts/ai_bullshit/ai/">ai bullshit</a></li></ul>
      
      
      
      
      
    </div>
  </div>
</div>


      <footer class="footer">
        <div class="links">
          <a href="/">Home</a>
          <span class="divider">|</span>
          <a href="/posts">Posts</a>
          <span class="divider">|</span>
          <a href="/about">About</a>
        </div>
        <a class="btn--remix" target="_top" href="https://glitch.com/edit/#!/remix/glitch-hello-eleventy">
        <img src="https://cdn.glitch.com/605e2a51-d45f-4d87-a285-9410ad350515%2FLogo_Color.svg?v=1618199565140" alt="" /> 
        Remix on Glitch
        </a>
        <a class="btn--remix" target="_top" href="https://glitch.com/edit/#!/open-lab-notebook">
        Edit
        </a>
      </footer>
    
  </main>
   
  </body>
</html>
