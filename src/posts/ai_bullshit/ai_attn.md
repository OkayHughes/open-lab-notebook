---
date: 2023-09-01
tags:
  - posts
  - misc
eleventyNavigation:
  parent: ai bullshit
  key: super-resolution-attention
layout: layouts/post.njk
---

# Note: This was generated by deepseek r1. I'm tryibng to see if it's useful enough to teach me things. 

# Next Step Tutorial: Adding Attention Mechanisms to Diffusion-Based Super-Resolution

This tutorial extends our previous diffusion-based super-resolution model by incorporating attention mechanisms. We'll explain how attention works, why it's beneficial for diffusion models, and how to implement it in our existing framework.

## Enhanced Implementation with Attention Mechanisms

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define attention mechanism
class SelfAttention(nn.Module):
    """
    Self-attention mechanism that allows each position to attend to all positions
    in the feature map. This helps capture long-range dependencies.
    
    REF: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
    """
    def __init__(self, channels, size):
        super(SelfAttention, self).__init__()
        self.channels = channels
        self.size = size
        
        # Query, Key, Value projections
        self.query = nn.Conv2d(channels, channels // 8, kernel_size=1)
        self.key = nn.Conv2d(channels, channels // 8, kernel_size=1)
        self.value = nn.Conv2d(channels, channels, kernel_size=1)
        
        # Gamma parameter (learnable weight for residual connection)
        self.gamma = nn.Parameter(torch.zeros(1))
        
        # Softmax for attention weights
        self.softmax = nn.Softmax(dim=-1)
        
    def forward(self, x):
        """
        Forward pass of self-attention.
        
        Args:
            x: Input tensor of shape (batch, channels, height, width)
            
        Returns:
            Output tensor with attention applied
        """
        batch_size, C, H, W = x.shape
        
        # Generate queries, keys, values
        # REF: Q = W_Q * x, K = W_K * x, V = W_V * x
        Q = self.query(x).view(batch_size, -1, H * W).permute(0, 2, 1)  # (B, N, C')
        K = self.key(x).view(batch_size, -1, H * W)  # (B, C', N)
        V = self.value(x).view(batch_size, -1, H * W)  # (B, C, N)
        
        # Calculate attention scores
        # REF: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V
        attention = torch.bmm(Q, K)  # (B, N, N)
        attention = attention / math.sqrt(self.channels // 8)  # Scale by sqrt(d_k)
        attention = self.softmax(attention)  # (B, N, N)
        
        # Apply attention to values
        out = torch.bmm(V, attention.permute(0, 2, 1))  # (B, C, N)
        out = out.view(batch_size, C, H, W)  # (B, C, H, W)
        
        # Residual connection with learnable weight
        # REF: Output = Î³ * Attention(Q, K, V) + Input
        out = self.gamma * out + x
        
        return out

# Define the enhanced U-Net with attention
class UNetWithAttention(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, base_channels=32):
        super(UNetWithAttention, self).__init__()
        
        # Encoder (downsampling)
        self.enc1 = self._block(in_channels, base_channels)
        self.enc2 = self._block(base_channels, base_channels*2)
        self.pool = nn.MaxPool2d(2)
        
        # Bottleneck with attention
        self.bottleneck = self._block(base_channels*2, base_channels*4)
        
        # Add attention mechanism at bottleneck
        # REF: Attention is most beneficial at lower resolutions where global context matters
        self.attention = SelfAttention(base_channels*4, size=(8, 8))
        
        # Decoder (upsampling)
        self.upconv2 = nn.ConvTranspose2d(base_channels*4, base_channels*2, kernel_size=2, stride=2)
        self.dec2 = self._block(base_channels*4, base_channels*2)  # Skip connection doubles channels
        self.upconv1 = nn.ConvTranspose2d(base_channels*2, base_channels, kernel_size=2, stride=2)
        self.dec1 = self._block(base_channels*2, base_channels)  # Skip connection doubles channels
        
        # Final output layer
        self.out = nn.Conv2d(base_channels, out_channels, kernel_size=1)
        
        # Time embedding layers
        self.time_embed = nn.Sequential(
            nn.Linear(1, base_channels),
            nn.SiLU(),
            nn.Linear(base_channels, base_channels)
        )
        
    def _block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1),
            nn.GroupNorm(8, out_channels),  # GroupNorm works better than BatchNorm for small batches
            nn.SiLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1),
            nn.GroupNorm(8, out_channels),
            nn.SiLU(inplace=True)
        )
    
    def forward(self, x, t):
        # Time embedding (improved)
        # REF: Project time to higher dimension and add to feature maps
        t_embed = self.time_embed(t.view(-1, 1)).unsqueeze(-1).unsqueeze(-1)
        
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        
        # Bottleneck with attention
        bottleneck = self.bottleneck(self.pool(enc2))
        
        # Add time embedding to bottleneck features
        bottleneck = bottleneck + t_embed.expand_as(bottleneck)
        
        # Apply attention
        bottleneck = self.attention(bottleneck)
        
        # Decoder with skip connections
        dec2 = self.upconv2(bottleneck)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.dec2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.dec1(dec1)
        
        return self.out(dec1)

# Define the diffusion process (same as before)
class DiffusionProcess:
    def __init__(self, T=1000, beta_start=1e-4, beta_end=0.02):
        self.T = T
        
        # Linear noise schedule
        self.betas = torch.linspace(beta_start, beta_end, T)
        self.alphas = 1 - self.betas
        self.alpha_bars = torch.cumprod(self.alphas, dim=0)
        
    def forward_process(self, x0, t):
        """Add noise to image at timestep t"""
        sqrt_alpha_bar = torch.sqrt(self.alpha_bars[t]).view(-1, 1, 1, 1).to(x0.device)
        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bars[t]).view(-1, 1, 1, 1).to(x0.device)
        epsilon = torch.randn_like(x0)
        
        xt = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * epsilon
        return xt, epsilon
    
    def reverse_process(self, model, x, t, condition=None):
        """Remove noise from image at timestep t using the model"""
        with torch.no_grad():
            # Predict the noise
            epsilon_pred = model(x, t)
            
            # Calculate coefficients for reverse process
            alpha_t = self.alphas[t].view(-1, 1, 1, 1).to(x.device)
            alpha_bar_t = self.alpha_bars[t].view(-1, 1, 1, 1).to(x.device)
            beta_t = self.betas[t].view(-1, 1, 1, 1).to(x.device)
            
            if t > 0:
                z = torch.randn_like(x)
            else:
                z = 0
                
            sigma_t = torch.sqrt(beta_t)
            x_prev = (1 / torch.sqrt(alpha_t)) * (x - (beta_t / torch.sqrt(1 - alpha_bar_t)) * epsilon_pred) + sigma_t * z
            
            return x_prev

# Training function (modified for attention model)
def train_diffusion_with_attention(model, diffusion, dataloader, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for i, (hr_imgs, _) in enumerate(dataloader):
            hr_imgs = hr_imgs.to(device)
            
            # Create low-resolution version (condition)
            lr_imgs = F.interpolate(hr_imgs, scale_factor=0.25, mode='bilinear')
            lr_imgs = F.interpolate(lr_imgs, size=hr_imgs.shape[2:], mode='bilinear')
            
            # Random timestep
            t = torch.randint(0, diffusion.T, (hr_imgs.size(0),), device=device).float()
            
            # Add noise to high-resolution images
            noisy_imgs, true_noise = diffusion.forward_process(hr_imgs, t)
            
            # Concatenate low-resolution condition with noisy image
            model_input = torch.cat([noisy_imgs, lr_imgs], dim=1)
            
            # Predict noise
            pred_noise = model(model_input, t)
            
            # Calculate loss
            loss = F.mse_loss(pred_noise, true_noise)
            
            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            if i % 100 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Batch {i}, Loss: {loss.item():.4f}")
        
        print(f"Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/len(dataloader):.4f}")

# Sampling function (same as before)
def sample_diffusion(model, diffusion, lr_img, img_size=(32, 32)):
    model.eval()
    
    # Start from random noise
    x = torch.randn(1, 3, *img_size).to(device)
    
    # Upsample low-resolution image to target size
    lr_img = F.interpolate(lr_img, size=img_size, mode='bilinear')
    
    # Reverse process
    for t in range(diffusion.T-1, -1, -1):
        # Create tensor of current timestep
        t_tensor = torch.tensor([t], device=device).float()
        
        # Concatenate with condition
        model_input = torch.cat([x, lr_img], dim=1)
        
        # Reverse process step
        x = diffusion.reverse_process(model, model_input, t_tensor)
        
        # Clamp values to valid range
        x = torch.clamp(x, -1.0, 1.0)
        
    return x

# Compare models function
def compare_models(original_model, attention_model, diffusion, test_img):
    """Compare the performance of original and attention models"""
    # Create low-resolution version
    lr_img = F.interpolate(test_img, scale_factor=0.25, mode='bilinear')
    lr_img = F.interpolate(lr_img, size=test_img.shape[2:], mode='bilinear')
    
    # Generate super-resolution images
    print("Generating with original model...")
    sr_original = sample_diffusion(original_model, diffusion, lr_img, img_size=test_img.shape[2:])
    
    print("Generating with attention model...")
    sr_attention = sample_diffusion(attention_model, diffusion, lr_img, img_size=test_img.shape[2:])
    
    # Visualize results
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))
    
    axes[0].imshow(test_img[0].cpu().permute(1, 2, 0) * 0.5 + 0.5)
    axes[0].set_title("Original HR Image")
    axes[0].axis('off')
    
    axes[1].imshow(lr_img[0].cpu().permute(1, 2, 0) * 0.5 + 0.5)
    axes[1].set_title("LR Input")
    axes[1].axis('off')
    
    axes[2].imshow(sr_original[0].cpu().permute(1, 2, 0) * 0.5 + 0.5)
    axes[2].set_title("Original Model")
    axes[2].axis('off')
    
    axes[3].imshow(sr_attention[0].cpu().permute(1, 2, 0) * 0.5 + 0.5)
    axes[3].set_title("Attention Model")
    axes[3].axis('off')
    
    plt.tight_layout()
    plt.savefig('comparison_result.png')
    plt.show()

# Main execution
if __name__ == "__main__":
    # Hyperparameters
    batch_size = 32
    learning_rate = 1e-4
    epochs = 5
    T = 1000  # Number of diffusion steps
    
    # Load and preprocess data
    transform = transforms.Compose([
        transforms.Resize((32, 32)),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # Initialize models and diffusion process
    original_model = UNet(in_channels=6, out_channels=3).to(device)
    attention_model = UNetWithAttention(in_channels=6, out_channels=3).to(device)
    diffusion = DiffusionProcess(T=T)
    
    # Train both models
    print("Training original model...")
    optimizer_original = torch.optim.Adam(original_model.parameters(), lr=learning_rate)
    train_diffusion(original_model, diffusion, train_loader, optimizer_original, epochs=epochs)
    
    print("Training attention model...")
    optimizer_attention = torch.optim.Adam(attention_model.parameters(), lr=learning_rate)
    train_diffusion_with_attention(attention_model, diffusion, train_loader, optimizer_attention, epochs=epochs)
    
    # Test with a sample image
    test_img, _ = next(iter(train_loader))
    test_img = test_img[0:1].to(device)  # Take one image
    
    # Compare models
    compare_models(original_model, attention_model, diffusion, test_img)
```

## Tutorial: Understanding Attention Mechanisms in Diffusion Models

### 1. Introduction to Attention Mechanisms

Attention mechanisms allow models to dynamically focus on different parts of their input when making predictions. In the context of diffusion models for super-resolution, attention helps the model:

1. **Focus on relevant regions**: Identify which parts of the low-resolution image need more refinement
2. **Capture long-range dependencies**: Understand relationships between distant pixels
3. **Handle varying levels of detail**: Adaptively process different image regions based on their complexity

### 2. Mathematical Foundation of Self-Attention

The self-attention mechanism is based on three learned projections:

#### Query, Key, and Value

For an input feature map \(X \in \mathbb{R}^{C \times H \times W}\):

\[
Q = W_Q \cdot X, \quad K = W_K \cdot X, \quad V = W_V \cdot X
\]

Where:
- \(W_Q, W_K, W_V\) are learned weight matrices
- \(Q, K, V\) are the query, key, and value representations

#### Attention Calculation

The attention weights are computed as:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

Where:
- \(d_k\) is the dimension of the key vectors (used for scaling)
- The softmax ensures the attention weights sum to 1

**Code Reference**: In the `SelfAttention.forward` method:
```python
# Calculate attention scores
attention = torch.bmm(Q, K)  # (B, N, N)
attention = attention / math.sqrt(self.channels // 8)  # Scale by sqrt(d_k)
attention = self.softmax(attention)  # (B, N, N)

# Apply attention to values
out = torch.bmm(V, attention.permute(0, 2, 1))  # (B, C, N)
```

#### Residual Connection

To ensure stable training, we use a residual connection:

\[
\text{Output} = \gamma \cdot \text{Attention}(Q, K, V) + X
\]

Where \(\gamma\) is a learnable parameter that starts at 0 and is gradually learned during training.

**Code Reference**: In the `SelfAttention.forward` method:
```python
# Residual connection with learnable weight
out = self.gamma * out + x
```

### 3. Integrating Attention into the Diffusion Model

#### Placement of Attention Blocks

Attention is most effective when placed in the bottleneck of the U-Net, where:
1. The feature maps are at their lowest spatial resolution
2. The receptive field is largest
3. Global context is most important

**Code Reference**: In the `UNetWithAttention` class:
```python
# Bottleneck with attention
self.bottleneck = self._block(base_channels*2, base_channels*4)

# Add attention mechanism at bottleneck
self.attention = SelfAttention(base_channels*4, size=(8, 8))
```

#### Enhanced Time Embedding

We've also improved the time embedding by:
1. Projecting the time step to a higher dimension
2. Using SiLU activation functions for smoother gradients
3. Adding the time embedding to the feature maps

**Code Reference**: In the `UNetWithAttention` class:
```python
# Time embedding layers
self.time_embed = nn.Sequential(
    nn.Linear(1, base_channels),
    nn.SiLU(),
    nn.Linear(base_channels, base_channels)
)

# Add time embedding to bottleneck features
bottleneck = bottleneck + t_embed.expand_as(bottleneck)
```

### 4. Benefits of Attention in Diffusion Models

#### Improved Coherence

Attention helps maintain coherence across the generated image by allowing different regions to "communicate" with each other. This is particularly important for super-resolution, where we need to ensure that the enhanced details are consistent across the entire image.

#### Better Handling of Complex Patterns

Attention mechanisms excel at capturing complex, long-range dependencies that are difficult for convolutional networks to learn. This makes them particularly well-suited for:

1. **Texture synthesis**: Generating realistic textures that match the input
2. **Edge preservation**: Maintaining sharp edges while reducing artifacts
3. **Detail enhancement**: Adding plausible high-frequency details

#### Adaptive Processing

Different image regions require different levels of processing. Attention allows the model to:

1. **Focus on difficult areas**: Spend more computational resources on complex regions
2. **Ignore simple areas**: Process uniform regions more efficiently
3. **Balance global and local information**: Combine both types of information effectively

### 5. Practical Implementation Considerations

#### Computational Cost

Attention mechanisms increase computational requirements:
- Memory: \(O(N^2)\) where N is the number of pixels
- Computation: \(O(N^2)\) for the attention matrix multiplication

**Mitigation strategies**:
1. Use attention only at lower resolutions
2. Implement efficient attention variants (e.g., linear attention)
3. Use attention in the bottleneck only

#### Training Stability

Attention can sometimes make training less stable. We address this by:
1. Using residual connections with learnable weights
2. Initializing the gamma parameter to 0
3. Using GroupNorm instead of BatchNorm

**Code Reference**: In the `UNetWithAttention._block` method:
```python
nn.GroupNorm(8, out_channels),  # GroupNorm works better than BatchNorm for small batches
```

### 6. Expected Improvements

With the addition of attention mechanisms, we expect to see:

1. **Higher quality results**: More realistic and detailed super-resolved images
2. **Better coherence**: Improved consistency across the generated image
3. **Reduced artifacts**: Fewer unnatural patterns or distortions
4. **Faster convergence**: More efficient learning of complex patterns

### 7. Further Enhancements

Beyond the basic attention mechanism implemented here, you could explore:

1. **Multi-head attention**: Allow the model to focus on different aspects simultaneously
2. **Cross-attention**: Specifically attend to the low-resolution conditioning image
3. **Efficient attention variants**: Reduce computational requirements
4. **Attention at multiple scales**: Apply attention at different levels of the U-Net

This enhanced implementation demonstrates how attention mechanisms can be integrated into diffusion models for super-resolution, providing both theoretical understanding and practical implementation details.
